{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618854aa-8df4-40d7-b956-638642501e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28b3319-fea6-4aa8-8da8-d3c88a21d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2bafad-be25-4e89-87d6-9234a12dcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3b5f1b-da64-4cca-944a-19e4f69bb5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"myothiha/jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c4e458-ed92-4624-9926-cb1ec48ed34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187641"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ds['train']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5033d2e9-182a-4cc1-bc3e-37b1c5eb6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<|endoftext|>'.join(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374a5ff0-ca55-4e96-82cf-a4c00b6d8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "context_length = 32\n",
    "n_embs = 128\n",
    "n_heads = 16\n",
    "n_blocks = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1678e521-f353-49cb-bd00-070a8844052d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20160500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf0edb3-4b5f-417b-b9d6-03f225622f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader():\n",
    "    def __init__(self, text, context_length, tokenizer, batch_size=1, device='cpu'):\n",
    "        v = tokenizer(text, return_tensors='pt')\n",
    "        self.tokens = v.input_ids # The attention mask will be handled manually later\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "        \n",
    "        self.position = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.reset()\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        B, T = self.batch_size, self.context_length\n",
    "        if self.position + B * T + 1 < len(self.tokens[0]):\n",
    "            tokens = self.tokens[0][self.position: self.position + B * T + 1]\n",
    "            self.position += B * T + 1\n",
    "            x = tokens[:-1].view(B, T)\n",
    "            y = tokens[1:].view(B, T)\n",
    "            return x.to(self.device), y.to(self.device)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.tokens[0]) // (self.context_length + 1) // self.batch_size\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bff33-94ea-4d42-8d21-d533aec74ef2",
   "metadata": {},
   "source": [
    "The `head_size` matches that of the embeddings if it is a single head.\n",
    "\n",
    "If multi-headed attention is used, then the `head_size` would equal number of embeddings // number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03339711-a3e5-47a4-923d-4f077210e85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.k(x) # (B, T, head_size)\n",
    "        q = self.q(x) # (B, T, head_size)\n",
    "        v = self.v(x) # (B, T, head_size)\n",
    "        \n",
    "#         attn = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, T)\n",
    "        \n",
    "#         wei = attn.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # Causal masking, blocks future tokens from being seen\n",
    "#         wei = wei.softmax(dim=-1) # (B, T, T)\n",
    "        \n",
    "#         out = wei @ v # (B, T, head_size)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b400d5-aa6a-4b65-b8d6-dc914cda5587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(n_embs // n_heads) for i in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embs, n_embs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92b6718-a1ae-4ea6-bcc5-93455857d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embs, n_embs * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embs * 4, n_embs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fe11e4-11a7-446a-9460-b06516801701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embs)\n",
    "        self.mha = MultiHeadAttention(n_heads)\n",
    "        self.ln2 = nn.LayerNorm(n_embs)\n",
    "        self.ffwd = FeedForward()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mha(self.ln1(x)) + x\n",
    "        x = self.ffwd(self.ln2(x)) + x\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f0c74a-c31f-4d47-993d-4ee10741d6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tk_emb = nn.Embedding(vocab_size, n_embs)\n",
    "        self.pos_emb = nn.Embedding(context_length, n_embs)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock() for i in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embs)\n",
    "        self.fc = nn.Linear(n_embs, vocab_size)\n",
    "        \n",
    "        self.tk_emb.weight = self.fc.weight\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        tk_emb = self.tk_emb(x) # (B, T, C)\n",
    "        pos_tns = torch.arange(T, device=device) # T\n",
    "        pos_emb = self.pos_emb(pos_tns) # (T, C)\n",
    "        x = pos_emb + tk_emb # (B, T, C) + (T, C)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.fc(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(B * T, -1), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735c13f3-b7b2-41c9-bf40-122942bd1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(mod):\n",
    "    if isinstance(mod, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(mod.weight)\n",
    "        if mod.bias is not None:\n",
    "            torch.nn.init.zeros_(mod.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c67e5f02-78a0-4730-979d-1f31dbab1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e125af26-a85c-4b2d-96d8-0eb3cd1f50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = TextDataLoader(text, context_length, tokenizer, batch_size=32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b7d3942-19bd-41ab-b090-21be4608e86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4425, 4673544)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl), len(dl.tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3388084f-a00e-4a00-91d7-c6e11018c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8070609"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT().to(device)\n",
    "model.apply(initialize)\n",
    "lr = 7e-4\n",
    "opt = optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2143b478-bf90-4f10-bf0f-eb2416bf035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.8622, -0.5032, -0.6921,  ..., -0.0037,  2.5393, -0.5748],\n",
       "          [ 1.6149, -0.0974,  0.0567,  ...,  0.3497,  2.0110, -1.3228],\n",
       "          [ 1.3989,  0.4020, -0.7424,  ...,  0.2187,  2.0540, -0.8872],\n",
       "          ...,\n",
       "          [ 0.7197,  1.0320, -0.9566,  ...,  0.9805,  3.1071,  0.2059],\n",
       "          [ 2.3216,  0.5853,  0.4516,  ...,  0.8154,  2.2397,  0.2956],\n",
       "          [ 2.8537,  0.1806, -0.0131,  ...,  0.7891,  2.1111, -0.5451]]],\n",
       "        device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor(11.9861, device='cuda:0', grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(dl))\n",
    "model(x=xb, targets=yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1598d46-3f90-4117-94b1-2cc27438b18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tk_emb): Embedding(50257, 128)\n",
       "  (pos_emb): Embedding(32, 128)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-15): 16 x SelfAttentionHead(\n",
       "            (k): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (q): Linear(in_features=128, out_features=8, bias=False)\n",
       "            (v): Linear(in_features=128, out_features=8, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=128, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a182f-e2a0-42c2-bd90-86e2b2a19ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 11.863006591796875\n",
      "Epoch: 0, Step: 442, Loss: 6.758306980133057\n",
      "Epoch: 0, Step: 884, Loss: 5.988088607788086\n",
      "Epoch: 0, Step: 1326, Loss: 5.640953540802002\n",
      "Epoch: 0, Step: 1768, Loss: 5.642059326171875\n",
      "Epoch: 0, Step: 2210, Loss: 5.361952304840088\n",
      "Epoch: 0, Step: 2652, Loss: 5.21062707901001\n"
     ]
    }
   ],
   "source": [
    "# Training loop for pre-training\n",
    "epochs = 1\n",
    "sched = CosineAnnealingLR(opt, epochs * len(dl), lr * 0.5)\n",
    "for i in range(epochs):\n",
    "    for step, (xb, yb) in enumerate(dl):\n",
    "        opt.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(xb, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        if step % (len(dl)//10) == 0 or step == len(dl):\n",
    "            print(f\"Epoch: {i}, Step: {step}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc4809-41e4-4bcb-ba06-bca688f547f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_tokens):\n",
    "    model.eval()\n",
    "    tokens = idx\n",
    "    for i in range(max_tokens):\n",
    "        logits = model(tokens[:, -context_length:])\n",
    "        topk_values, topk_indices = torch.topk(logits[:, -1, :], 50)\n",
    "        probs = topk_values.softmax(dim=-1)\n",
    "        sample = torch.multinomial(probs, 1)\n",
    "        token = torch.gather(topk_indices, 1, sample)\n",
    "        tokens = torch.cat((tokens, token), dim=-1)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a0d07-88e0-40f8-8af6-b49a900f3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = tokenizer.encode(\"What do you call\", return_tensors='pt').to(device)\n",
    "print(tokenizer.decode(generate(start_tokens, 2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9309d-a870-49ac-9a94-2c3228e4c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"models/jokes_transformer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
