{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618854aa-8df4-40d7-b956-638642501e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor, nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28b3319-fea6-4aa8-8da8-d3c88a21d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c2bafad-be25-4e89-87d6-9234a12dcfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89971e7a613495b80e6692c70d61674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4325692a887e449993db1a7d7e2ed415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6d9c49a97945b79217d540b48161db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd64685967b84da88ef36f35aa268c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4d091b23de422ba263b9976223d89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"openai-community/gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3b5f1b-da64-4cca-944a-19e4f69bb5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaa6abeef0140fcba817cd002647378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb051103d9c46978ec7f2c70ddc662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/19.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b45040c08b24761818018530bb5a5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566a3d3fb4ea4a9abad84e17df2be6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc678b40996348feb60f0966f0bd6aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/187641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111950a5876a4c929c1c7d7e14446c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/20850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7784fec669cc48b2ae3713298eb9eb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/23166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"myothiha/jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c4e458-ed92-4624-9926-cb1ec48ed34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187641"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ds['train']['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5033d2e9-182a-4cc1-bc3e-37b1c5eb6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<|endoftext|>'.join(ds['train']['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374a5ff0-ca55-4e96-82cf-a4c00b6d8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "context_length = 32\n",
    "n_embs = 128\n",
    "n_heads = 16\n",
    "n_blocks = 8\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1678e521-f353-49cb-bd00-070a8844052d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20160500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf0edb3-4b5f-417b-b9d6-03f225622f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataLoader():\n",
    "    def __init__(self, text, context_length, tokenizer, batch_size=1, device='cpu', mask = False):\n",
    "        v = tokenizer(text, return_tensors='pt')\n",
    "        self.tokens = v.input_ids # The attention mask will be handled manually later\n",
    "        self.masks =  torch.ones(batch_size, context_length) # for finetuning huggingface models\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "        self.device = device\n",
    "        \n",
    "        self.position = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.reset()\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        B, T = self.batch_size, self.context_length\n",
    "        if self.position + B * T + 1 < len(self.tokens[0]):\n",
    "            tokens = self.tokens[0][self.position: self.position + B * T + 1]\n",
    "            self.position += B * T + 1\n",
    "            x = tokens[:-1].view(B, T)\n",
    "            y = tokens[1:].view(B, T)\n",
    "            if self.masks is not None:\n",
    "                return x.to(self.device), self.masks.to(self.device), y.to(self.device)\n",
    "            return x.to(self.device), y.to(self.device)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.tokens[0]) // (self.context_length + 1) // self.batch_size\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73bff33-94ea-4d42-8d21-d533aec74ef2",
   "metadata": {},
   "source": [
    "The `head_size` matches that of the embeddings if it is a single head.\n",
    "\n",
    "If multi-headed attention is used, then the `head_size` would equal number of embeddings // number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03339711-a3e5-47a4-923d-4f077210e85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.k = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.q = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.v = nn.Linear(n_embs, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.k(x) # (B, T, head_size)\n",
    "        q = self.q(x) # (B, T, head_size)\n",
    "        v = self.v(x) # (B, T, head_size)\n",
    "        \n",
    "#         attn = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, T)\n",
    "        \n",
    "#         wei = attn.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # Causal masking, blocks future tokens from being seen\n",
    "#         wei = wei.softmax(dim=-1) # (B, T, T)\n",
    "        \n",
    "#         out = wei @ v # (B, T, head_size)\n",
    "        out = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b400d5-aa6a-4b65-b8d6-dc914cda5587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttentionHead(n_embs // n_heads) for i in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embs, n_embs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92b6718-a1ae-4ea6-bcc5-93455857d8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embs, n_embs * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_embs * 4, n_embs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fe11e4-11a7-446a-9460-b06516801701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embs)\n",
    "        self.mha = MultiHeadAttention(n_heads)\n",
    "        self.ln2 = nn.LayerNorm(n_embs)\n",
    "        self.ffwd = FeedForward()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mha(self.ln1(x)) + x\n",
    "        x = self.ffwd(self.ln2(x)) + x\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f0c74a-c31f-4d47-993d-4ee10741d6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tk_emb = nn.Embedding(vocab_size, n_embs)\n",
    "        self.pos_emb = nn.Embedding(context_length, n_embs)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock() for i in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embs)\n",
    "        self.fc = nn.Linear(n_embs, vocab_size)\n",
    "        \n",
    "        self.tk_emb.weight = self.fc.weight\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape\n",
    "        tk_emb = self.tk_emb(x) # (B, T, C)\n",
    "        pos_tns = torch.arange(T, device=device) # T\n",
    "        pos_emb = self.pos_emb(pos_tns) # (T, C)\n",
    "        x = pos_emb + tk_emb # (B, T, C) + (T, C)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.fc(x) # (B, T, vocab_size)\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(B * T, -1), targets.view(B*T))\n",
    "            return logits, loss\n",
    "        \n",
    "    def generate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "735c13f3-b7b2-41c9-bf40-122942bd1c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(mod):\n",
    "    if isinstance(mod, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(mod.weight)\n",
    "        if mod.bias is not None:\n",
    "            torch.nn.init.zeros_(mod.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c67e5f02-78a0-4730-979d-1f31dbab1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e125af26-a85c-4b2d-96d8-0eb3cd1f50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4673544 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "dl = TextDataLoader(text, context_length, tokenizer, batch_size=16, device=device, mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b7d3942-19bd-41ab-b090-21be4608e86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8851"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3388084f-a00e-4a00-91d7-c6e11018c3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8232bd0bb604224b4865276cc9838db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5704f7836b1d4b979c9413c2caaf5d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "163087441"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = GPT().to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.lm_head = nn.Linear(768, 50257).to(device)\n",
    "# model.apply(initialize)\n",
    "lr = 7e-4\n",
    "opt = optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2143b478-bf90-4f10-bf0f-eb2416bf035d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.7118, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, masks, yb = next(iter(dl))\n",
    "model(input_ids=xb, attention_mask=masks, labels=yb).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1598d46-3f90-4117-94b1-2cc27438b18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a57b393c-397f-4379-b027-fae968c0cf5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 18.450794219970703\n",
      "Epoch: 0, Step: 442, Loss: 6.331844806671143\n",
      "Epoch: 0, Step: 884, Loss: 5.9788737297058105\n",
      "Epoch: 0, Step: 1326, Loss: 5.828938007354736\n",
      "Epoch: 0, Step: 1768, Loss: 5.4010844230651855\n",
      "Epoch: 0, Step: 2210, Loss: 5.53565788269043\n",
      "Epoch: 0, Step: 2652, Loss: 5.770430088043213\n",
      "Epoch: 0, Step: 3094, Loss: 5.153104782104492\n",
      "Epoch: 0, Step: 3536, Loss: 5.821743488311768\n",
      "Epoch: 0, Step: 3978, Loss: 5.305891036987305\n",
      "Epoch: 0, Step: 4420, Loss: 5.067082405090332\n",
      "Epoch: 0, Step: 4862, Loss: 5.330435752868652\n",
      "Epoch: 0, Step: 5304, Loss: 5.594406604766846\n",
      "Epoch: 0, Step: 5746, Loss: 5.049088954925537\n",
      "Epoch: 0, Step: 6188, Loss: 5.299722671508789\n",
      "Epoch: 0, Step: 6630, Loss: 5.385701656341553\n",
      "Epoch: 0, Step: 7072, Loss: 5.208510875701904\n",
      "Epoch: 0, Step: 7514, Loss: 4.577992916107178\n",
      "Epoch: 0, Step: 7956, Loss: 4.767080783843994\n",
      "Epoch: 0, Step: 8398, Loss: 5.327550411224365\n",
      "Epoch: 0, Step: 8840, Loss: 5.458425998687744\n",
      "Epoch: 0, Step: 8851, Loss: 5.0539374351501465\n"
     ]
    }
   ],
   "source": [
    "# Training loop for fine-tuning a huggingface model\n",
    "epochs = 1\n",
    "sched = CosineAnnealingLR(opt, epochs * len(dl), lr * 0.01)\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for step, (xb, mask, yb) in enumerate(dl):\n",
    "        opt.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            output = model(input_ids=xb, attention_mask=mask, labels=yb)\n",
    "            loss = output.loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        if step % (len(dl)//20) == 0 or step == len(dl):\n",
    "            print(f\"Epoch: {i}, Step: {step}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64fc4809-41e4-4bcb-ba06-bca688f547f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_tokens):\n",
    "    model.eval()\n",
    "    tokens = idx\n",
    "    for i in range(max_tokens):\n",
    "        logits = model(tokens[:, -context_length:])\n",
    "        topk_values, topk_indices = torch.topk(logits[:, -1, :], 50)\n",
    "        probs = topk_values.softmax(dim=-1)\n",
    "        sample = torch.multinomial(probs, 1)\n",
    "        token = torch.gather(topk_indices, 1, sample)\n",
    "        tokens = torch.cat((tokens, token), dim=-1)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaab3bea-70e1-4539-8384-acaa1ddb2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_finetune(input_ids, attention_mask, max_tokens=50):\n",
    "    model.eval()\n",
    "    tokens = input_ids\n",
    "    for i in range(max_tokens):\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        topk_values, topk_indices = torch.topk(logits[:, -1, :], 10)\n",
    "        probs = topk_values.softmax(dim=-1)\n",
    "        sample = torch.multinomial(probs, 1)\n",
    "        token = torch.gather(topk_indices, 1, sample)\n",
    "        tokens = torch.cat((tokens, token), dim=-1)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "256a9b12-79a0-4fef-8936-c872a405a1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you call black gay black when black guy guy Mexican dog gay when Mexican who Mexican black when when dog group gay when when black black black who when when guy when Mexican gay cow when gay who Mexican when black when black group cow black when Mexican black when cow when group when when gay black when Mexican who black when gay when dog group group who black who black group when who when guy when when Mexican when cow black when when black when when black black gay black who who Mexican guy Mexican when when dog group when when\n"
     ]
    }
   ],
   "source": [
    "start_tokens = tokenizer(\"What do you call\", return_tensors='pt').to(device)\n",
    "print(tokenizer.decode(generate_finetune(**start_tokens, max_tokens=100)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
